{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_SAMPLES = 35998\n",
    "NUM_VAL_SAMPLES = 5368\n",
    "\n",
    "VERBOSE = True\n",
    "MIN_DELTA = 0.005\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "\n",
    "MODEL_NAME = 'simple_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_image_dataset = tf.data.TFRecordDataset('data/train.tfrecord')\n",
    "#\n",
    "# Create a dictionary describing the features.\n",
    "#image_feature_description = {\n",
    "#    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "#    'angle': tf.io.FixedLenFeature([], tf.float32),\n",
    "#    'throttle': tf.io.FixedLenFeature([], tf.float32)\n",
    "#}\n",
    "#\n",
    "#def _parse_image_function(example_proto):\n",
    "#  # Parse the input tf.Example proto using the dictionary above.\n",
    "#  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "#\n",
    "#parsed_image_dataset = raw_image_dataset.map(_parse_image_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_jpeg(image_buffer, scope=None):\n",
    "    \"\"\"Decode a JPEG string into one 3-D float image Tensor.\n",
    "    Args:\n",
    "        image_buffer: scalar string Tensor.\n",
    "        scope: Optional scope for name_scope.\n",
    "    Returns:\n",
    "        3-D float Tensor with values ranging from [0, 1).\n",
    "    \"\"\"\n",
    "    with tf.name_scope(values=[image_buffer], name=scope,\n",
    "                       default_name='decode_jpeg'):\n",
    "        # Decode the string as an RGB JPEG.\n",
    "        # Note that the resulting image contains an unknown height\n",
    "        # and width that is set dynamically by decode_jpeg. In other\n",
    "        # words, the height and width of image is unknown at compile-i\n",
    "        # time.\n",
    "        image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "\n",
    "        # After this point, all image pixels reside in [0,1)\n",
    "        # until the very end, when they're rescaled to (-1, 1).\n",
    "        # The various adjust_* ops all require this range for dtype\n",
    "        # float.\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        return image\n",
    "\n",
    "def _parse_fn(example_serialized, is_training=False):\n",
    "    \"\"\" ...\n",
    "    \"\"\"\n",
    "    feature_map = {\n",
    "        'image': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "        'angle': tf.FixedLenFeature([], dtype=tf.float32, default_value=0.0),\n",
    "        'throttle': tf.FixedLenFeature([], dtype=tf.float32, default_value=0.0),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(example_serialized, feature_map)\n",
    "    image = decode_jpeg(parsed['image'])\n",
    "    image = tf.reshape(image, (1, 240, 360, 3))\n",
    "    return (image, (parsed['angle'], parsed['throttle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tfrecords_dir, subset, batch_size):\n",
    "    \"\"\"Read TFRecords files and turn them into a TFRecordDataset.\"\"\"\n",
    "    files = tf.matching_files(os.path.join(tfrecords_dir, '%s-*' % subset))\n",
    "    shards = tf.data.Dataset.from_tensor_slices(files)\n",
    "    shards = shards.shuffle(tf.cast(tf.shape(files)[0], tf.int64))\n",
    "    shards = shards.repeat()\n",
    "    dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=4)\n",
    "    dataset = dataset.shuffle(buffer_size=8192)\n",
    "    parser = partial(\n",
    "        _parse_fn, is_training=True if subset == 'train' else False)\n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            map_func=parser,\n",
    "            batch_size=batch_size,\n",
    "            num_parallel_calls=config.NUM_DATA_WORKERS))\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b69f9b405428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-54ea38e89ebe>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(tfrecords_dir, subset, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     parser = partial(\n\u001b[0m\u001b[1;32m     10\u001b[0m         _parse_fn, is_training=True if subset == 'train' else False)\n\u001b[1;32m     11\u001b[0m     dataset = dataset.apply(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "get_dataset('data', 'train', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset('data/train.tfrecord')\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_validation_set = tf.data.TFRecordDataset('data/val.tfrecord')\n",
    "parsed_validation_set = raw_image_dataset.map(_parse_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import Convolution2D, Convolution3D\n",
    "from tensorflow.python.keras.layers import MaxPooling2D, MaxPooling3D\n",
    "from tensorflow.python.keras.activations import relu\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.python.keras.layers import Cropping2D, Cropping3D\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_loss_angle = 0.9\n",
    "weight_loss_throttle = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_model(img_dims, crop_margin_from_top=80):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    img_in = Input(shape=(img_dims), name='img_in')\n",
    "\n",
    "    x = img_in\n",
    "\n",
    "    x = Cropping2D(((crop_margin_from_top, 0), (0, 0)))(x)\n",
    "\n",
    "    # Define convolutional neural network to extract features from the images\n",
    "    x = Convolution2D(filters=24, kernel_size=(5, 5), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(filters=24, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(filters=24, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(filters=24, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(filters=24, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #x = Convolution2D(filters=32, kernel_size=(5, 5), strides=(2, 2), activation='relu')(x)\n",
    "    #x = Convolution2D(filters=64, kernel_size=(5, 5), strides=(2, 2), activation='relu')(x)\n",
    "    #x = Convolution2D(filters=64, kernel_size=(3, 3), strides=(2, 2), activation='relu')(x)\n",
    "    #x = Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu')(x)\n",
    "\n",
    "    # Define decision layers to predict steering and throttle\n",
    "    x = Flatten(name='flattened')(x)\n",
    "    #x = Dense(units=100, activation='linear')(x)\n",
    "    #x = Dropout(rate=.5)(x)\n",
    "    x = Dense(units=10, activation='linear')(x)\n",
    "    x = Dropout(rate=.5)(x)\n",
    "    # categorical output of the angle\n",
    "    angle_out = Dense(units=1, activation='linear', name='angle_out')(x)\n",
    "\n",
    "    # continous output of throttle\n",
    "    throttle_out = Dense(units=1, activation='linear', name='throttle_out')(x)\n",
    "\n",
    "    model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss={'angle_out': 'mean_squared_error',\n",
    "                      'throttle_out': 'mean_squared_error'},\n",
    "                loss_weights={'angle_out': weight_loss_angle,\n",
    "                              'throttle_out': weight_loss_throttle},\n",
    "                metrics=['mse', 'mae', 'mape'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_in (InputLayer)             (None, 240, 360, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 160, 360, 3)  0           img_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 156, 356, 24) 1824        cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 78, 178, 24)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 76, 176, 24)  5208        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 38, 88, 24)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 36, 86, 24)   5208        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 18, 43, 24)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 41, 24)   5208        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 20, 24)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 6, 18, 24)    5208        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 9, 24)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flattened (Flatten)             (None, 648)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           6490        flattened[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "angle_out (Dense)               (None, 1)            11          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "throttle_out (Dense)            (None, 1)            11          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 29,168\n",
      "Trainable params: 29,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_2d_model(img_dims=[240, 360, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model will be stored to: ./simple_model.h5\n"
     ]
    }
   ],
   "source": [
    "#TODO: based on running locally or valohai the dir should be changed\n",
    "#saved_model_path = os.path.join(datadir, 'models', model_file_name)\n",
    "outputs_dir = os.getenv('VH_OUTPUTS_DIR', './')\n",
    "output_file = os.path.join(outputs_dir, '%s.h5' % MODEL_NAME)\n",
    "\n",
    "\n",
    "print('model will be stored to: %s' % output_file)\n",
    "\n",
    "# checkpoint to save model after each epoch\n",
    "save_best = ModelCheckpoint(output_file,\n",
    "                            monitor='val_loss',\n",
    "                            verbose=VERBOSE,\n",
    "                            save_best_only=True,\n",
    "                            mode='min')\n",
    "\n",
    "# stop training if the validation error stops improving.\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=MIN_DELTA,\n",
    "                           patience=PATIENCE,\n",
    "                           verbose=VERBOSE,\n",
    "                           mode='auto')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "json_callback = LambdaCallback(on_epoch_end = lambda epoch, logs: json.dumps({'loss': logs['loss'],\n",
    "                                                                             'val_loss': logs['val_loss']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(dict({'loss': logs['loss'], 'val_loss': logs['val_loss']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 49ms/step - loss: 6.2118e-04 - angle_out_loss: 3.3477e-04 - throttle_out_loss: 0.0032 - angle_out_mean_squared_error: 3.3477e-04 - angle_out_mean_absolute_error: 0.0183 - angle_out_mean_absolute_percentage_error: 18296830.0000 - throttle_out_mean_squared_error: 0.0032 - throttle_out_mean_absolute_error: 0.0566 - throttle_out_mean_absolute_percentage_error: 185324.2656\n",
      "{'loss': 0.0024564333315538534, 'val_loss': 0.0006211796426214278}\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 5.4519e-04 - angle_out_loss: 3.2375e-04 - throttle_out_loss: 0.0025 - angle_out_mean_squared_error: 3.2375e-04 - angle_out_mean_absolute_error: 0.0180 - angle_out_mean_absolute_percentage_error: 17993176.0000 - throttle_out_mean_squared_error: 0.0025 - throttle_out_mean_absolute_error: 0.0504 - throttle_out_mean_absolute_percentage_error: 165079.0000\n",
      "{'loss': 0.002103773888666183, 'val_loss': 0.0005451898323372006}\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 4.7405e-04 - angle_out_loss: 3.1286e-04 - throttle_out_loss: 0.0019 - angle_out_mean_squared_error: 3.1286e-04 - angle_out_mean_absolute_error: 0.0177 - angle_out_mean_absolute_percentage_error: 17687790.0000 - throttle_out_mean_squared_error: 0.0019 - throttle_out_mean_absolute_error: 0.0439 - throttle_out_mean_absolute_percentage_error: 143757.1562\n",
      "{'loss': 0.002705061389133334, 'val_loss': 0.0004740520962513983}\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 4.0238e-04 - angle_out_loss: 2.8333e-04 - throttle_out_loss: 0.0015 - angle_out_mean_squared_error: 2.8333e-04 - angle_out_mean_absolute_error: 0.0168 - angle_out_mean_absolute_percentage_error: 16832276.0000 - throttle_out_mean_squared_error: 0.0015 - throttle_out_mean_absolute_error: 0.0384 - throttle_out_mean_absolute_percentage_error: 125794.7031\n",
      "{'loss': 0.0021638067613821478, 'val_loss': 0.00040237733628600836}\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 3.6549e-04 - angle_out_loss: 2.7359e-04 - throttle_out_loss: 0.0012 - angle_out_mean_squared_error: 2.7359e-04 - angle_out_mean_absolute_error: 0.0165 - angle_out_mean_absolute_percentage_error: 16540622.0000 - throttle_out_mean_squared_error: 0.0012 - throttle_out_mean_absolute_error: 0.0345 - throttle_out_mean_absolute_percentage_error: 113154.7656\n",
      "{'loss': 0.001659867388661951, 'val_loss': 0.00036548683419823647}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(parsed_image_dataset,\n",
    "                    validation_data = parsed_image_dataset,\n",
    "                    steps_per_epoch = 10,\n",
    "                    validation_steps = 5,\n",
    "                    batch_size=8,\n",
    "                    epochs=5,\n",
    "                    callbacks=[JsonLogger()],\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 150ms/step - loss: 0.0200 - angle_out_loss: 0.0186 - throttle_out_loss: 0.0323 - angle_out_mean_squared_error: 0.0186 - angle_out_mean_absolute_error: 0.1364 - angle_out_mean_absolute_percentage_error: 136413728.0000 - throttle_out_mean_squared_error: 0.0323 - throttle_out_mean_absolute_error: 0.1798 - throttle_out_mean_absolute_percentage_error: 589255.0000\n",
      "140/140 [==============================] - 29s 207ms/step - loss: 0.0875 - angle_out_loss: 0.0895 - throttle_out_loss: 0.0696 - angle_out_mean_squared_error: 0.0895 - angle_out_mean_absolute_error: 0.2512 - angle_out_mean_absolute_percentage_error: 213741760.0000 - throttle_out_mean_squared_error: 0.0696 - throttle_out_mean_absolute_error: 0.2364 - throttle_out_mean_absolute_percentage_error: 570195.3125 - val_loss: 0.0200 - val_angle_out_loss: 0.0186 - val_throttle_out_loss: 0.0323 - val_angle_out_mean_squared_error: 0.0186 - val_angle_out_mean_absolute_error: 0.1364 - val_angle_out_mean_absolute_percentage_error: 136413728.0000 - val_throttle_out_mean_squared_error: 0.0323 - val_throttle_out_mean_absolute_error: 0.1798 - val_throttle_out_mean_absolute_percentage_error: 589255.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(parsed_image_dataset,\n",
    "          validation_data = parsed_image_dataset,\n",
    "          steps_per_epoch = NUM_TRAIN_SAMPLES // BATCH_SIZE,\n",
    "          validation_steps = NUM_VAL_SAMPLES // BATCH_SIZE,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=1) #EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
